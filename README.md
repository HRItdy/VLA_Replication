# VLA_Replication
The UR5 script for data collection and evaluation, with the record of common issues encountered


## Dataset conversion
We used Diffusion policy to collect data, and use... to convert ....
rld5 repo provides conversion from ... to ...
Pi0 provide libero to...

## RDT-1b
Unified state vector

## $\pi_0$
client

directly use finetuned model on libero

## Diffusion Policy
https://github.com/real-stanford/diffusion_policy

1. Comply with the hardware requirements. Then run `diffusion_policy/diffusion_policy/demo_real_robot.py` to teleoperate UR5 with SpaceMouse.

2. The data converting the collected dataset to replaybuffer is in `diffusion_policy/diffusion_policy/real_world/real_data_conversion.py`. The structure is like:
![Screenshot from 2025-05-24 14-27-57](https://github.com/user-attachments/assets/ad8eedb2-1614-470c-9792-650f9b1aece3)

3. We provide a script to convert .zarr dataset to h5py dataset: `diffusion_policy/diffusion_policy/real_world/dataset_conversion.py`.

4. This code uses yaml to recursively define the task. Please change yaml in task to aligh it with your own dataset, and policy relevant yaml to define the policy to call.

5. We provide `visualize.py` to visualize the video in the converted dataset, help users to identify whether the converted dataset is consistant with the original one. Meanwhile, `wandb_drawer.py` is provided to users to consolidate the training results from different wandb projects and json files into one paper-style figure.
## Action Chunking Transformer
https://github.com/Shaka-Labs/ACT
1. First align the dimension of `state_dim` of vae and the training dataet.
   When installed ACT you should also installed `detr`, go to `detr->models->detr_vae.py`, change line 230 to the demension of your training dataset.

   Another way to open this is directing to `training->policy.py', go to definition of `build_ACT_model_and_optimizer->build_ACT_model->build_vae`.

2. Second need to padding all the episodes to the fixed length, which is defined in `ACT/config/config.py` as `episode_len` in `TASK_CONFIG`. Please run the code in `dataset_preproc.py` under ACT folder, which will pad all of your episodes in the dataset to the predefined `episode_len`.

3. Put the dataset with **hdf5** format under `data` folder
```
├── data
│   └── pick_screwdriver
│       ├── screwdriver
│       │   ├── episode_0.hdf5
│       │   ├── episode_1.hdf5
│       │   ├── episode_2.hdf5
│       │   ├── ...
│       └── text_embed
│           └── embed_0.pt
```
Forget whether need to put text embedding inside (generated by feed the task instruction like `pick up the screwdriver` into t5). If some error meaning the number of episodes is not correct raises, then delete it.

4. Run `python train.py --task sort`. Change `sort` to the name of your dataset folder (`pick_screwdriver` in my case).

## Evaluation
1. We provide `UR5_test_ACT.py`, `UR5_test_openvla.py` and `UR5_test_openpi.py` for onsite inference. Expect for ACT, other two models please refer to the previous sections to identify how to establish the server.

